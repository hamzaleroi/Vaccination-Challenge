{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "colab_type": "code",
    "id": "pfVats5LgdYK",
    "outputId": "613ea4ea-84eb-43bc-b8e2-ff6ba178ec63"
   },
   "source": [
    "# This cell can be trasnformed into a code cell in order to download the datasets if they're not in the workin\n",
    "!wget https://s3.amazonaws.com/drivendata-prod/data/66/public/training_set_features.csv\n",
    "!wget https://s3.amazonaws.com/drivendata-prod/data/66/public/submission_format.csv\n",
    "!wget https://s3.amazonaws.com/drivendata-prod/data/66/public/training_set_labels.csv\n",
    "!wget https://s3.amazonaws.com/drivendata-prod/data/66/public/test_set_features.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PJ13XLOHNFSr"
   },
   "source": [
    "## Importing the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3BRvOUI4gos3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import  f1_score, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HxPH8xyRNHit"
   },
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f-2UOwLRhEqw"
   },
   "outputs": [],
   "source": [
    "path_train_X='./training_set_features.csv'\n",
    "path_train_Y='./training_set_labels.csv'\n",
    "path_test_X='./test_set_features.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "hFGdEiZIhXAZ",
    "outputId": "43bce4a8-65bf-4446-9eb1-bc7a4236d5be",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(path_train_X)\n",
    "print(train_set.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "G2R_9ib1hchQ",
    "outputId": "8e9c8844-e5f6-4418-8519-95e4e1b5446c"
   },
   "outputs": [],
   "source": [
    "train_set_target = pd.read_csv(path_train_Y)\n",
    "print(train_set_target.head())\n",
    "print(train_set_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "HrcVv4AGhih5",
    "outputId": "312713df-7883-4f95-c63d-b6b886633c75"
   },
   "outputs": [],
   "source": [
    "test_set_X = pd.read_csv(path_test_X)\n",
    "test_set_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "dNZIR6AUhv8V",
    "outputId": "c12eee2f-40a3-46b2-a3f9-3c734fa22e21",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "d47wbyArh1w2",
    "outputId": "90633ad6-e0cd-4626-ee8b-a5689951f369",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_set_X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting columns with too mant missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = train_set.isnull().sum()\n",
    "columns = list(sums[sums > 0.4 * train_set.shape[0]].keys())\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_set.drop(columns=columns)\n",
    "df = df.drop(columns=['respondent_id',])\n",
    "df.isnull().sum()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to predict with rows with that doesn't have any mising values:  \n",
    "Here I tried to  train a model on 2 different datasets: one achieved through imputaionand the other through deleting missing values if at least one exist in a given row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set2 = pd.concat((df,train_set_target),axis=1).dropna(axis=0)\n",
    "train_set2.shape, '<----',df.shape, 'null values :' , train_set2.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = train_set2.drop(train_set_target.columns,axis=1)\n",
    "df2_target = train_set2[train_set_target.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing the others with the most frequent value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impt = SimpleImputer(missing_values=np.nan,strategy='most_frequent')\n",
    "df = pd.DataFrame(impt.fit_transform(df),columns=df.columns)\n",
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using LabelEncoder to label the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "classes= {}\n",
    "for col in df.columns:\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    df2[col] = le.fit_transform(df2[col])\n",
    "    classes[col] = le.classes_\n",
    "classes    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting non-binary features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns_categorical=[]\n",
    "for col in df.columns[1:]:\n",
    "    if len(df[col].unique()) > 2:\n",
    "        columns_categorical.append(col)\n",
    "columns_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OHE for the non-binary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(df,columns=columns_categorical)\n",
    "X2 = pd.get_dummies(df2,columns=columns_categorical)\n",
    "X.shape,X2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction using PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "num = (pca.explained_variance_ratio_*100).astype(int).sum()\n",
    "pca = PCA(n_components=num)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "X_reduced[:5,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X2)\n",
    "num = (pca.explained_variance_ratio_*100).astype(int).sum()\n",
    "pca = PCA(n_components=num)\n",
    "X_reduced2 = pca.fit_transform(X2)\n",
    "X_reduced2[:5,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First dealing with seasonal vaccine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_seasonal = train_set_target['seasonal_vaccine'].to_numpy().ravel()\n",
    "Y_seasonal2 = df2_target['seasonal_vaccine'].to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Train/Valid/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_valid,Y_train,Y_valid = train_test_split(X,Y_seasonal,test_size=.1,random_state=102,stratify=Y_seasonal)\n",
    "X_train2,X_valid2,Y_train2,Y_valid2 = train_test_split(X2,Y_seasonal2,test_size=.1,random_state=102,stratify=Y_seasonal2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a basic model on the both datasets: \n",
    "This a model with minimum parameters.The classification metric used in the f1 score which is a more accurate metric, other than the regular \"accuracy\" metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 77.62% ,  valid: 76.55%\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(max_iter=500,random_state=101)\n",
    "logreg.fit(X_train2,Y_train2)\n",
    "print('train: {:.2f}% , '.format(f1_score(logreg.predict(X_train2),Y_train2)*100) , \\\n",
    "'valid: {:.2f}%'.format(f1_score(logreg.predict(X_valid2),Y_valid2)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 75.80% ,  valid: 76.77%\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(max_iter=500,random_state=101)\n",
    "logreg.fit(X_train,Y_train)\n",
    "print('train: {:.2f}% , '.format(f1_score(logreg.predict(X_train),Y_train)*100) , \\\n",
    "'valid: {:.2f}%'.format(f1_score(logreg.predict(X_valid),Y_valid)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "We can clearly see that there isn't much of a difference concerning the validation accuracy. That's why it's better to stick with real data than the ones manipulated through imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying a logistic regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter=1001,random_state=101,solver='liblinear',penalty='l1')\n",
    "logreg.fit(X_train,Y_train)\n",
    "logreg.score(X_train,Y_train)*100, '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(logreg.predict(X_valid),Y_valid)*100,'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(classification_report(logreg.predict(X_valid),Y_valid,output_dict = True)).T*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only keeping the important columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = abs(logreg.coef_ )  > 1e-5\n",
    "mask = mask.ravel()\n",
    "mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = X_reduced[:,mask]\n",
    "X_train,X_valid,Y_train,Y_valid = train_test_split(X_reduced,Y_seasonal,test_size=.1,random_state=102,stratify=Y_seasonal)\n",
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter=1001,random_state=101,solver='liblinear',penalty='l1')\n",
    "logreg.fit(X_train,Y_train)\n",
    "'{:.2f}'.format(logreg.score(X_train,Y_train)*100) , '{:.2f}'.format(f1_score(logreg.predict(X_valid),Y_valid)*100),'%'"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "vaccinaton_challenge_preprocessing.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
